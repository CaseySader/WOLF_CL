Before this project began, the WOLF framework was largely in place, so modifications could readily be made. The first few months of work were spent going through the WOLF code to understand the workflow and be able to add in new features. When this project began, Sohaib Kiani was working on adding a neural network model that used Caffe. My role involved helping to read in and use the hyper-parameters the user provided that were to be tested. To make changes in WOLF, it is required to learn about the complete workflow to ensure understanding of what each file in the WOLF framework does and how data is passed amongst them. Work on new features could begin once the framework was understood.

The website was planned and worked on in a team setting. Goals on what would be desired in the website and how it should generally be set up were discussed. The main goal was for the version of WOLF created by Bahl \parencite{WOLFprojectreport} to have every feature in the website. To be able to select every model and hyper-parameter combination, a database needed to be set up and populated with initial values. It also needed to store information on users and all runs of WOLF. The database was my contribution.

Once the work became just this project, existing tools and experiences about machine learning were analyzed to determine what would be wanted out of WOLF that was not there already. The first task was in determining if there was a better way to implement the neural network. The documentation for Caffe \parencite{Caffe}, PyTorch \parencite{PyTorch} and TensorFlow \parencite{TensorFlow} were studied. It was decided that the best framework to use would be TensorFlow due to its popularity and the ability to use Keras. The reason for Keras is the capability to make a dynamic model without having to reinvent the wheel.

Once all the models were integrated in that were wanted, the project moved on to saving trained models. For this, it is most common to save the models as a pickle file and the user can load that up in another file if they wish to use it. Because of how WOLF trains models, there are going to be multiple models trained for each data split. All of these models are saved into a folder specific to the model type and hyper-parameters and the results excel file provides a file path to the best model for those hyper-parameters based on a chosen metric.

After models were successfully saved, another transaction type was created that allows for choosing some data and a model to use and then WOLF outputs predicted values to a file. This is done by reading in the data and ensuring it is in the same format as the data that trained the model. Then the model is loaded using pickle and a function is called to predict the label. The predictions are then written to a file that the user will have access to.

Calculating feature importance was done in a similar way as saving a model. Each model has its own way of determining the best features, but they all had to be put in a standardized output. For some models, there is a {\tt feature\_importances\_} attribute in {\tt scikit-learn} \parencite{sklearn} that provides a value on each parameter. For other models a {\tt coef\_} attribute is used and then the values one is interested in are taken out. For the Neural Network, ELI5 \parencite{eli5} can be used to determine the feature importances by replacing the values of a dataset feature with random values and seeing how it affects the model accuracy. This is helpful because instead of having to take out a feature and then retrain, we can keep the same model and just make predictions to see how each feature affects the model output.

I then took benchmark datasets and ran them in WOLF to both show my understanding and allow proof of WOLF's ability for a conference \parencite{WOLFpresentation}. The datasets are listed in the Presentation of Work section and the results of these runs can be seen in the Results section.

Finally, some practical aspects were also explored, such as obtaining a reservation on the cluster, using different priorities, selecting processor types (CPU and GPU), choosing how many of each processor would be used, and monitoring the order of transactions.