When making any tool, there are always going to be features one wishes they had time to implement. Here are some that have already been started or that have been talked about as something that is needed.

\section*{Website}
Work on a website version of WOLF has begun and is nearly deployable. It currently allows the loading of a dataset, selection of which transactions to run, and selection of the hyper-parameters to test on. It currently only fails in signaling that the run is complete and providing the results back to the user, which was not a part of the website I was assigned to work on. However, the results file is created and can be found.

There is also the possibility of adding more website-specific features such as visualization or the ability to work on a workflow as a group. The current website and SQL database were created with the intention of WOLF being like a social network for running machine learning models. If this is still the goal, then the current setup could be used and completed. If this is not the case, then they could be used as a reference for a brand new project.

My recommendation would be to use the UI as a visual reference either way and switch from the existing PHP code to JavaScript for its popularity and security. Tracking the run and updating the database and website also needs to be looked into as that is the main problem right now. The cause is that there is something wrong with the watcher files stored in a jar file. The files in here are Java binaries, and we do not have the source code. As for the SQL database, it is a good database to start with for functionality, but more security features should be looked into, including adding increased security through Linux permissions.

\section*{Feature importance}
After completion of this project, for feature importance WOLF lists the features and a number in a sorted format to show the order of the relative importance of the features to the selected model. This is better than nothing, but should be improved upon. Visualizations would be one way to do this. Another way to improve would involve looking into more ways to calculate feature importance in the dataset, e.g., explained variance \parencite{HandsOn}. 

ELI5 was used for the neural network, but it does have the capability to run on Scikit-Learn models. Looking into if it is better to use ELI5 instead of, or in addition to, the existing {\tt sklearn} methods would be a beneficial exploratory task. It would provide more information to the user and multiple perspectives that could confirm the idea of which features are the most important.

\section*{Local runs}
Currently to run WOLF, a user must have an ITTC account and then run on the ITTC cluster by submitting the job through the login servers. This is fine for KU users, but requires an internet connection, space on the cluster to be requested, and alienates non-KU users. A working website would remove the need to be affiliated with KU, but still does not solve the need for internet or the potential for a long wait on the cluster. Having a version that can be downloaded and run locally using both a user's CPU and GPU would be an optimal solution to this problem.

To make WOLF open source and reproducible, the first thing that would need to be changed is the submission scripts which have the templates in {\tt Wolf.py} for the cluster. These would need to be changed to run on a local machine. I have not attempted this too deeply, but one idea is that the cluster configuration section before the python calls could be removed and it might work by just creating scripts that make {\tt python} calls. This would need to be tested for Windows, Mac, and Linux. Chances are this might work for Mac and Linux, but not Windows. It would also need to be looked into if the dependencies that {\tt Wolf.py} sets up for the submission script running order is consistent with a standard terminal or only the cluster. There would also be a need to setup a WOLF MongoDB environment for the user and have all of the necessary packages installed using {\tt pip}. A script could be created to do both of these tasks. It may also be necessary to create separate versions of WOLF for different operating systems. Another area that could be beneficial would be to create an executable that could be downloaded and run by a user of WOLF. This could either be an executable that installs everything for them, or could even be used to run WOLF for users that do not care about editing or knowing about anything except the configuration files.

Another option that could be looked into is using Docker \parencite{Docker}. It is an open-source software tool that allows for applications to be built in Docker containers and shared as one package. It can then be run on any Linux machine making Docker similar to running WOLF in a virtual machine. There are also versions of Docker for creating containers for Windows and Mac.

\section*{Image data}
One of the most popular uses of machine learning is image recognition, but WOLF currently does not support this. One of the main reasons is the need for a dataset to be uploaded as an {\tt arff} file that WOLF then uses to split and create {\tt csv} files. Having a way to signify that the data will be images and handle that appropriately would be important to the future success of WOLF.

This could just entail having the feature values in the {\tt arff} file be the names of each image, but this would need to be tested thoroughly and my suspicions are that WOLF would not support this technique, especially for calculating feature importances. Alternatively, my recommendation would be to create a key in the {\tt yaml} file which can be chosen for image data upon which a different method for pre-processing and datasplitting will be performed. This could either involve creating features from each pixel in the image or, maybe best case, finding a way to use the file names of the images in the {\tt csv} files from the data splits. If each pixel value is used to create a feature instead of using the filename, I assume it will be required that the images in the dataset are the exact same dimensions.
\section*{Python 3}
Support for Python 2.7 will end on January 1, 2020 and because of this, any improvements to WOLF should be made in Python 3 and all existing code should be ported over to Python 3. It can be assumed that when Python 2.7 support ends, Scikit-Learn will also stop any support for new features for the 2.7 version. There are already warning messages that some functions will be depreciated in future versions of Scikit-Learn. It is not a priority for WOLF in its current state and the code will still work, but with any improvements, this should be done.

The changes needed would mainly be syntax changes and any function calls that have changed in the module version updates. It would also be needed to make sure that python 3, the Scikit-Learn module, and the TensorFlow for gpu module is updated on the ITTC cluster. Finally, all {\tt python} calls in the submission script templates found in {\tt Wolf.py} would need to be changed to {\tt python3} or however the cluster support chooses to have the call at that time.

\section*{More Model Types}
WOLF currently contains some of the most popular machine learning models today, but it is not a complete list, even for classification. Adding any more would be a bonus. There is also a whole area of work that needs to be added for regression. Some of the Scikit-Learn models have the ability to perform either classification or regression, such as linear regression, logistic regression, and random forest, but most that are currently in WOLF are only setup to perform classification. There is a metrics python file, {\tt MetricCollection\_Regression.py}, that has been started for regression as well, but it is not in a state to be put in fully to WOLF yet.

\section*{GPU Reservation}
In the current state of the ITTC cluster, there are a small number of GPUs. This makes it difficult to run WOLF using neural networks if the cluster is even remotely busy. One potential fix would be to gain access to a reservation of GPUs that only WOLF can access. Another potential fix would be to have more GPUs be added to the cluster.